{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome Skele-Friend! \ud83d\udc80\u2728 \u00b6 This is the official and most up-to-date place to find documentation for FreeMoCap. We're slowly building a Knowledge Base that roughly follows the 'diataxis framework' . Our documentation is very much a work in progress, so we appreciate your patience, support, and engagement! If you're looking for a quick start, head on over to our \"How to\" Guides page! These docs are a work in progress! Better tutorials/walkthroughs/etc coming soon! We are very close to out v0.1.0 release, and there will be a new round of tutorials/walk-throughs/etc released around then. In the mean time, check out this (rough) video which provides a broad overview of some of the topics relevant to camera-based markerless motion capture ( HINT - Look at the video chapters for specific topics. ) Helpful Links \u00b6 The FreeMoCap Website https://freemocap.org The FreeMoCap GitHub https://github.com/freemocap/freemocap Support FreeMoCap by donating to our non-profit that supports our work! Troubleshooting? \u00b6 If you run into an issue using the software itself, post an issue on our GitHub, here: https://github.com/freemocap/freemocap/issues If there's an error in our documentation, post an issue on our documentation repository, here: https://github.com/freemocap/documentation/issues Join the Discord and ask a question in the #help-requests channel. Click here to join our Discord","title":"Welcome Skele-Friend! \ud83d\udc80\u2728"},{"location":"#welcome-skele-friend","text":"This is the official and most up-to-date place to find documentation for FreeMoCap. We're slowly building a Knowledge Base that roughly follows the 'diataxis framework' . Our documentation is very much a work in progress, so we appreciate your patience, support, and engagement! If you're looking for a quick start, head on over to our \"How to\" Guides page! These docs are a work in progress! Better tutorials/walkthroughs/etc coming soon! We are very close to out v0.1.0 release, and there will be a new round of tutorials/walk-throughs/etc released around then. In the mean time, check out this (rough) video which provides a broad overview of some of the topics relevant to camera-based markerless motion capture ( HINT - Look at the video chapters for specific topics. )","title":"Welcome Skele-Friend! \ud83d\udc80\u2728"},{"location":"#helpful-links","text":"The FreeMoCap Website https://freemocap.org The FreeMoCap GitHub https://github.com/freemocap/freemocap Support FreeMoCap by donating to our non-profit that supports our work!","title":"Helpful Links"},{"location":"#troubleshooting","text":"If you run into an issue using the software itself, post an issue on our GitHub, here: https://github.com/freemocap/freemocap/issues If there's an error in our documentation, post an issue on our documentation repository, here: https://github.com/freemocap/documentation/issues Join the Discord and ask a question in the #help-requests channel. Click here to join our Discord","title":"Troubleshooting?"},{"location":"about_us/","text":"About Us \u00b6 The Free Motion Capture Project (FreeMoCap) aims to provide research-grade markerless motion capture software to everyone for free. We're building a user-friendly framework that connects an array of bleeding edge open-source tools from the computer vision and machine learning communities to accurately record full-body 3D movement of humans, animals, robots, and other objects. We want to make the newly emerging mind-boggling, future-shaping technologies that drive FreeMoCap's core functionality accessible to communities of people who stand to benefit from them. We follow a \u201cUniversal Design\u201d development philosophy, with the goal of creating a system that serves the needs of a professional research scientist while remaining intuitive to a 13-year-old with no technical training and no outside assistance. A high-quality, minimal-cost motion capture system would be a transformative tool for a wide range of communities - including 3d animators, game designers, athletes, coaches, performers, scientists, engineers, clinicians, and doctors. We hope to create a system that brings new technological capacity to these groups while also building bridges between them. Everyone has a reason to record human movement We want to help them do it \u2728\ud83d\udc80\u2728 (copied from https://freemocap.org )","title":"About Us"},{"location":"about_us/#about-us","text":"The Free Motion Capture Project (FreeMoCap) aims to provide research-grade markerless motion capture software to everyone for free. We're building a user-friendly framework that connects an array of bleeding edge open-source tools from the computer vision and machine learning communities to accurately record full-body 3D movement of humans, animals, robots, and other objects. We want to make the newly emerging mind-boggling, future-shaping technologies that drive FreeMoCap's core functionality accessible to communities of people who stand to benefit from them. We follow a \u201cUniversal Design\u201d development philosophy, with the goal of creating a system that serves the needs of a professional research scientist while remaining intuitive to a 13-year-old with no technical training and no outside assistance. A high-quality, minimal-cost motion capture system would be a transformative tool for a wide range of communities - including 3d animators, game designers, athletes, coaches, performers, scientists, engineers, clinicians, and doctors. We hope to create a system that brings new technological capacity to these groups while also building bridges between them. Everyone has a reason to record human movement We want to help them do it \u2728\ud83d\udc80\u2728 (copied from https://freemocap.org )","title":"About Us"},{"location":"frequently_asked_questions_faq/","text":"FAQ 's - Ongoing Draft \u00b6 Work in progress, you can find the WIP version of this page on the github repo (but no promises on its accuracy)","title":"FAQ's - Ongoing Draft"},{"location":"frequently_asked_questions_faq/#faqs-ongoing-draft","text":"Work in progress, you can find the WIP version of this page on the github repo (but no promises on its accuracy)","title":"FAQ's - Ongoing Draft"},{"location":"privacy_policy/","text":"Privacy Policy \u00b6 We respect and value your privacy. This Privacy Policy outlines how we collect, use, and protect any personal or anonymous data we may collect from users of our software. By using our software, you agree to the terms of this Privacy Policy. \"We\" refers to the FreeMoCap development team, which maintains the FreeMoCap software. \"You\" refers to the user of our software. User data helps us understand how we can make our software better for you and allows us to demonstrate to agencies and corporations that people are using our software, which may help us to grow the project in the future. Collection of Anonymous User Data \u00b6 If you check the \"Send User Pings\" check box on the main page of the GUI, we will collect anonymous user data when you use our software. This information is sent to us as a \"ping\" of data posted to pipedream every time the software is used. The data we currently collect includes: your IP Address (because the \"ping\" is sent to pipedream via a POST request, we cannot avoid collecting your IP address) The time the \"ping\" was sent information about your cameras You can view the code that collects this data here . If you do not wish to share your data, you may turn off \"pings\" at any point in time. To turn off user pings, uncheck the \"Send User Pings\" box on the home screen of the FreeMoCap GUI. Protection of User Data \u00b6 We take the protection of your data seriously and will not sell or distribute any personal or anonymous data we collect to third parties, except as required by law. We use industry-standard security measures to protect your data from unauthorized access, use, or disclosure. Currently, only core FreeMoCap developers (Jonathan Matthis, and Trenton Wirth) have access to the user data we have collected. Your Control Over Your Data \u00b6 As a user of our software, you have control over your data. You can choose to turn off \"pings\" at any point in time by unchecking the \"Send User Pings\" box on the home screen of the FreeMoCap GUI. If you wish to have your user data deleted, you may contact us at info AT freemocap DOT org. Updates to This Privacy Policy \u00b6 We may update this Privacy Policy as our project evolves, so please check back periodically for changes. Your continued use of our software after any changes to this Privacy Policy will constitute your acceptance of such changes. Contact Us \u00b6 If you have any questions or concerns about this Privacy Policy or our use of your data, please contact us at info AT freemocap DOT org, or reach out to us on our Discord.","title":"Privacy Policy"},{"location":"privacy_policy/#privacy-policy","text":"We respect and value your privacy. This Privacy Policy outlines how we collect, use, and protect any personal or anonymous data we may collect from users of our software. By using our software, you agree to the terms of this Privacy Policy. \"We\" refers to the FreeMoCap development team, which maintains the FreeMoCap software. \"You\" refers to the user of our software. User data helps us understand how we can make our software better for you and allows us to demonstrate to agencies and corporations that people are using our software, which may help us to grow the project in the future.","title":"Privacy Policy"},{"location":"privacy_policy/#collection-of-anonymous-user-data","text":"If you check the \"Send User Pings\" check box on the main page of the GUI, we will collect anonymous user data when you use our software. This information is sent to us as a \"ping\" of data posted to pipedream every time the software is used. The data we currently collect includes: your IP Address (because the \"ping\" is sent to pipedream via a POST request, we cannot avoid collecting your IP address) The time the \"ping\" was sent information about your cameras You can view the code that collects this data here . If you do not wish to share your data, you may turn off \"pings\" at any point in time. To turn off user pings, uncheck the \"Send User Pings\" box on the home screen of the FreeMoCap GUI.","title":"Collection of Anonymous User Data"},{"location":"privacy_policy/#protection-of-user-data","text":"We take the protection of your data seriously and will not sell or distribute any personal or anonymous data we collect to third parties, except as required by law. We use industry-standard security measures to protect your data from unauthorized access, use, or disclosure. Currently, only core FreeMoCap developers (Jonathan Matthis, and Trenton Wirth) have access to the user data we have collected.","title":"Protection of User Data"},{"location":"privacy_policy/#your-control-over-your-data","text":"As a user of our software, you have control over your data. You can choose to turn off \"pings\" at any point in time by unchecking the \"Send User Pings\" box on the home screen of the FreeMoCap GUI. If you wish to have your user data deleted, you may contact us at info AT freemocap DOT org.","title":"Your Control Over Your Data"},{"location":"privacy_policy/#updates-to-this-privacy-policy","text":"We may update this Privacy Policy as our project evolves, so please check back periodically for changes. Your continued use of our software after any changes to this Privacy Policy will constitute your acceptance of such changes.","title":"Updates to This Privacy Policy"},{"location":"privacy_policy/#contact-us","text":"If you have any questions or concerns about this Privacy Policy or our use of your data, please contact us at info AT freemocap DOT org, or reach out to us on our Discord.","title":"Contact Us"},{"location":"python_reference/","text":"Python Reference \u00b6 This'll be like, I don't know, the API or something?","title":"Python Reference"},{"location":"python_reference/#python-reference","text":"This'll be like, I don't know, the API or something?","title":"Python Reference"},{"location":"troubleshooting/","text":"Troubleshooting \u00b6 For fixing specific problems Installation problems \u00b6 Etc \u00b6","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"For fixing specific problems","title":"Troubleshooting"},{"location":"troubleshooting/#installation-problems","text":"","title":"Installation problems"},{"location":"troubleshooting/#etc","text":"","title":"Etc"},{"location":"explanations/Untitled/","text":"","title":"Untitled"},{"location":"how_to_guides/","text":"\"How to\" Guides \u00b6 Text and pictures \u00b6 How to use the pre-alpha \u00b6 How to install and run the alpha GUI \u00b6 How to process pre-recorded synchronized videos with alpha GUI \u00b6 Links to videos \u00b6 How to choose your cameras \u00b6 How to calibrate your capture volume \u00b6","title":"\"How to\" Guides"},{"location":"how_to_guides/#how-to-guides","text":"","title":"\"How to\" Guides"},{"location":"how_to_guides/#text-and-pictures","text":"","title":"Text and pictures"},{"location":"how_to_guides/#how-to-use-the-pre-alpha","text":"","title":"How to use the pre-alpha"},{"location":"how_to_guides/#how-to-install-and-run-the-alpha-gui","text":"","title":"How to install and run the alpha GUI"},{"location":"how_to_guides/#how-to-process-pre-recorded-synchronized-videos-with-alpha-gui","text":"","title":"How to process pre-recorded synchronized videos with alpha GUI"},{"location":"how_to_guides/#links-to-videos","text":"","title":"Links to videos"},{"location":"how_to_guides/#how-to-choose-your-cameras","text":"","title":"How to choose your cameras"},{"location":"how_to_guides/#how-to-calibrate-your-capture-volume","text":"","title":"How to calibrate your capture volume"},{"location":"how_to_guides/how_to_install_and_run_the_alpha_gui/","text":"How to install and run the alpha GUI \u00b6 This is a work-in-progress :D No promises here, skele-friends! The alpha is in active development and so this is likely to change very soon. If you'd like to use the alpha , Most of the dev team runs the GUI through PyCharm , but its easier to write instructions on how to run from an anaconda prompt. Pre-requisites: \u00b6 Install Anaconda https://anaconda.org Install git (just use the defaults) (OPTIONAL, but highly recommended) Install Blender - https://blender.org Installation instructions \u00b6 Open anaconda enabled terminal Create a python=3.9 environment conda create -n freemocap-gui python = 3 .9 Activate that environment: conda activate freemocap-gui Clone the repository (i.e. download the code from github. It'll show up in the current working directory of your terminal session) git clone https://github.com/freemocap/freemocap Navigate into that newly cloned/downloaded freemocap folder with: cd freemocap Install the dependencies listed in the requirements.txt file: pip install -r requirements.txt Run the GUI by running the src/gui/main/main.py file by entering this command into the terminal: python src/gui/main/main.py Hopefully a GUI popped up! There are no docs on usage yet, so just click and see what you can figure out Current limitations \u00b6 At the moment, the alpha GUI's method for connecting to the cameras is very innefficient and will experience framerate drops with more than ~3 cameras (even with a powerful PC). We're working on a fix, and should have it handled soon! In the mean time, you can still use the GUI to process videos recorded with other methods (workflow described in the next section!)","title":"How to install and run the `alpha` GUI"},{"location":"how_to_guides/how_to_install_and_run_the_alpha_gui/#how-to-install-and-run-the-alpha-gui","text":"This is a work-in-progress :D No promises here, skele-friends! The alpha is in active development and so this is likely to change very soon. If you'd like to use the alpha , Most of the dev team runs the GUI through PyCharm , but its easier to write instructions on how to run from an anaconda prompt.","title":"How to install and run the alpha GUI"},{"location":"how_to_guides/how_to_install_and_run_the_alpha_gui/#pre-requisites","text":"Install Anaconda https://anaconda.org Install git (just use the defaults) (OPTIONAL, but highly recommended) Install Blender - https://blender.org","title":"Pre-requisites:"},{"location":"how_to_guides/how_to_install_and_run_the_alpha_gui/#installation-instructions","text":"Open anaconda enabled terminal Create a python=3.9 environment conda create -n freemocap-gui python = 3 .9 Activate that environment: conda activate freemocap-gui Clone the repository (i.e. download the code from github. It'll show up in the current working directory of your terminal session) git clone https://github.com/freemocap/freemocap Navigate into that newly cloned/downloaded freemocap folder with: cd freemocap Install the dependencies listed in the requirements.txt file: pip install -r requirements.txt Run the GUI by running the src/gui/main/main.py file by entering this command into the terminal: python src/gui/main/main.py Hopefully a GUI popped up! There are no docs on usage yet, so just click and see what you can figure out","title":"Installation instructions"},{"location":"how_to_guides/how_to_install_and_run_the_alpha_gui/#current-limitations","text":"At the moment, the alpha GUI's method for connecting to the cameras is very innefficient and will experience framerate drops with more than ~3 cameras (even with a powerful PC). We're working on a fix, and should have it handled soon! In the mean time, you can still use the GUI to process videos recorded with other methods (workflow described in the next section!)","title":"Current limitations"},{"location":"how_to_guides/how_to_process_previously_recorded_videos/","text":"How to process pre-recorded synchronized videos with alpha GUI \u00b6 This is a workflow you can use to process any pre-recorded & synchronized videos. Note: Rebooting the GUI You can re-start the GUI at any time by pressing CTRl+R and then press Ctrl-D to reload the the most recent session Follow for pre-alpha Tips If you've pre-recorded videos using freemocap pre-alpha (<=v0.0.54) , these blue boxes with our little skele-friend will contain info to specifically address a \" pre-alpha -> alpha GUI \" workflow! Step 1 - Run the GUI \u00b6 One-click installation coming soon This software is still very much a work in progress and not quite as stable and performant as it could be. As soon as it is, you will be able to install and run the GUI from the freemocap website without the needing to crate python environments or run anything from the terminal/command line' Make sure you're using the most recent version of the freemocap software by entering the command git pull in the terminal before running main.py Prepare a python environment using these instructions from README on the freemocap GitHub repository . Once you have set up and activated your environment, run python src/gui/main/main.py in a terminal with the proper environment activated. A GUI should pop up that looks like this: Step 2 - Import Videos \u00b6 Select Import Synchronized Videos (or ctrl+I ). 2.1 - Create session_id \u00b6 The terminal is your friend! Throughout each step of this process, watch the terminal (the one you launched the GUI from) for valuable feedback about the processing steps and progress bars. This session_id is created based on the date and time that you began this session (format: session_YYYY-MM-DD_HH_MM_SS ) will be the name of the folder that will house the data from this session. It will be located in your user directory in a folder called freemocap_data/[session_id] Optional - add a tag to the session_id dialog to identify which videos you will import. Warning: Do not use spaces in the session_id , use underscores _ or dashes - to break up words instead. 2.2 - Select folder of synchronized videos \u00b6 pre-alpha tip If re-processing a pre-alpha session, select the [freemocap_data]/[session_id]/SyncedVideos folder Click Select set of synchronized videos... button. Select the folder full of synchronized ( .mp4 ) videos. Converting videos to .mp4 You can convert your videos to .mp4 format with Handbrake . Synchronize your own videos Videos should all have exactly the same number of frames and the frames should be synchronized with each other (for example, frame#120 of one camera should show an image from the same time-point as frame#120 of the other cameras) Where perfect synchronization is not possible, just do your best. As a rule of thumb, try to ensure that the frames from the different cameras within a single 'frame duration' of eachother (so +/- 33ms for 30fps videos) If your videos have an audio track (e.g. GoPro videos), you may be able to synchronize them using your preferred Video Editor. Step 3 - Calibration \u00b6 Open the 2-Capture Volume tab. Option #1 - Load the camera_calibration.toml from a prior freemocap session \u00b6 pre-alpha tip If re-processing a pre-alpha session, select the [freemocap_data]/[session_id]/[session_id]_camera_calibration.toml file Select Load Camera Calibration .toml file... option. Load in the ...camera_calibration.toml file from a previously recorded session. Option #2 - Process calibration videos in GUI \u00b6 With Calibrate 'from synchronized_videos' folder option selected, click Calibration Capture Volume from Videos button. Warning: Only select Option #2 if your videos include a Charuco Board calibration procedure. For more information about the charuco board and camera calibration, check out this (section of) this video. Step 4 - Process Videos \u00b6 Open the 3-Motion Capture Data tab. Set processing parameters however you like. Recommended - Install Blender Blender is the current best method to view and explore the data produced by the freemocap software. You can install blender, here . The base data will be loaded as a set of keyframed empty objects with names that match the landmarks tracked by the mediapipe 'holistic' solution , which roughly correspond to major joint centers. For help navigating in Blender, check out this video from @BlenderGuru. Warning: You may need to manually specify the location of the Blender executable in the GUI. using the Locate Blender Executable button in the Motion Capture Data panel Click Process All Steps Below button. Clicking this button will automatically run through every other option on this tab, and you wont need to click anything else! Step 5 - Visualize Recording \u00b6 If all went well, the GUI may have automatically opened Blender with your motion capture data pre-loaded, and you should be done! If not, double check to make sure the blender executable location is specified correctly. You can re-launch the Export to Blender process with the Generate '.blend' file button. You're Done! Next Steps? If you've gotten this far, you've most likely produced a visualization of your mocap! Hooray! What next? Join our Discord to share videos and screenshots, and to talk with devs if you need some help! Share your success on any social media platform with the #madewithfreemocap tag! Take a break, because you've earned it :D","title":"How to process pre-recorded synchronized videos with `alpha` GUI"},{"location":"how_to_guides/how_to_process_previously_recorded_videos/#how-to-process-pre-recorded-synchronized-videos-with-alpha-gui","text":"This is a workflow you can use to process any pre-recorded & synchronized videos. Note: Rebooting the GUI You can re-start the GUI at any time by pressing CTRl+R and then press Ctrl-D to reload the the most recent session Follow for pre-alpha Tips If you've pre-recorded videos using freemocap pre-alpha (<=v0.0.54) , these blue boxes with our little skele-friend will contain info to specifically address a \" pre-alpha -> alpha GUI \" workflow!","title":"How to process pre-recorded synchronized videos with alpha GUI"},{"location":"how_to_guides/how_to_process_previously_recorded_videos/#step-1-run-the-gui","text":"One-click installation coming soon This software is still very much a work in progress and not quite as stable and performant as it could be. As soon as it is, you will be able to install and run the GUI from the freemocap website without the needing to crate python environments or run anything from the terminal/command line' Make sure you're using the most recent version of the freemocap software by entering the command git pull in the terminal before running main.py Prepare a python environment using these instructions from README on the freemocap GitHub repository . Once you have set up and activated your environment, run python src/gui/main/main.py in a terminal with the proper environment activated. A GUI should pop up that looks like this:","title":"Step 1 - Run the GUI"},{"location":"how_to_guides/how_to_process_previously_recorded_videos/#step-2-import-videos","text":"Select Import Synchronized Videos (or ctrl+I ).","title":"Step 2 - Import Videos"},{"location":"how_to_guides/how_to_process_previously_recorded_videos/#21-create-session_id","text":"The terminal is your friend! Throughout each step of this process, watch the terminal (the one you launched the GUI from) for valuable feedback about the processing steps and progress bars. This session_id is created based on the date and time that you began this session (format: session_YYYY-MM-DD_HH_MM_SS ) will be the name of the folder that will house the data from this session. It will be located in your user directory in a folder called freemocap_data/[session_id] Optional - add a tag to the session_id dialog to identify which videos you will import. Warning: Do not use spaces in the session_id , use underscores _ or dashes - to break up words instead.","title":"2.1 - Create session_id"},{"location":"how_to_guides/how_to_process_previously_recorded_videos/#22-select-folder-of-synchronized-videos","text":"pre-alpha tip If re-processing a pre-alpha session, select the [freemocap_data]/[session_id]/SyncedVideos folder Click Select set of synchronized videos... button. Select the folder full of synchronized ( .mp4 ) videos. Converting videos to .mp4 You can convert your videos to .mp4 format with Handbrake . Synchronize your own videos Videos should all have exactly the same number of frames and the frames should be synchronized with each other (for example, frame#120 of one camera should show an image from the same time-point as frame#120 of the other cameras) Where perfect synchronization is not possible, just do your best. As a rule of thumb, try to ensure that the frames from the different cameras within a single 'frame duration' of eachother (so +/- 33ms for 30fps videos) If your videos have an audio track (e.g. GoPro videos), you may be able to synchronize them using your preferred Video Editor.","title":"2.2 - Select folder of synchronized videos"},{"location":"how_to_guides/how_to_process_previously_recorded_videos/#step-3-calibration","text":"Open the 2-Capture Volume tab.","title":"Step 3 - Calibration"},{"location":"how_to_guides/how_to_process_previously_recorded_videos/#step-4-process-videos","text":"Open the 3-Motion Capture Data tab. Set processing parameters however you like. Recommended - Install Blender Blender is the current best method to view and explore the data produced by the freemocap software. You can install blender, here . The base data will be loaded as a set of keyframed empty objects with names that match the landmarks tracked by the mediapipe 'holistic' solution , which roughly correspond to major joint centers. For help navigating in Blender, check out this video from @BlenderGuru. Warning: You may need to manually specify the location of the Blender executable in the GUI. using the Locate Blender Executable button in the Motion Capture Data panel Click Process All Steps Below button. Clicking this button will automatically run through every other option on this tab, and you wont need to click anything else!","title":"Step 4 - Process Videos"},{"location":"how_to_guides/how_to_process_previously_recorded_videos/#step-5-visualize-recording","text":"If all went well, the GUI may have automatically opened Blender with your motion capture data pre-loaded, and you should be done! If not, double check to make sure the blender executable location is specified correctly. You can re-launch the Export to Blender process with the Generate '.blend' file button. You're Done! Next Steps? If you've gotten this far, you've most likely produced a visualization of your mocap! Hooray! What next? Join our Discord to share videos and screenshots, and to talk with devs if you need some help! Share your success on any social media platform with the #madewithfreemocap tag! Take a break, because you've earned it :D","title":"Step 5 - Visualize Recording"},{"location":"how_to_guides/how_to_use_the_pre-alpha_code/","text":"How to use the pre-alpha \u00b6 We're in the process of launching our alpha ... We're switching over to the alpha phase of this project ( v0.1.0 and on) , which use full refactor code written with help from a professional experienced software architect. Until the new code stabilizes, you may have more luck using the pre-alpha code (e.g. v0.0.54 ). If you'd like to try and use the alpha GUI, you can start with this How-To-Guide . Installing the pre-alpha \u00b6 Note: Our pre-alpha is frozen This will install the latest & last version from the pre-alpha phase of this project, frozen at release tag v0.0.54 here Open an Anaconda-enabled command prompt or powershell window and enter the following commands: 1) Create a Python 3.7 Anaconda environment conda create -n freemocap-env python = 3 .7 2) Activate that newly created environment conda activate freemocap-env 3) Install freemocap (version 0.0.54 ) from PyPi using pip pip install freemocap == 0 .0.54 Warning: BUG FIX - Update mediapipe with pip install mediapipe --upgrade That should be it! How to create a new pre-alpha recording session \u00b6 tl;dr- Activate the freemocap Python environment and run the following lines of code (either in a script or in a console) import freemocap freemocap . RunMe () Be cool, use Blender COOL KIDS will install Blender ( blender.org and generate an awesome .blend file animation by setting useBlender=True : import freemocap freemocap . RunMe ( useBlender = True ) For additional, more detailed instructions (including methods to re-process recorded sessions), refer to the OLD_README.md document )","title":"How to use the `pre-alpha`"},{"location":"how_to_guides/how_to_use_the_pre-alpha_code/#how-to-use-the-pre-alpha","text":"We're in the process of launching our alpha ... We're switching over to the alpha phase of this project ( v0.1.0 and on) , which use full refactor code written with help from a professional experienced software architect. Until the new code stabilizes, you may have more luck using the pre-alpha code (e.g. v0.0.54 ). If you'd like to try and use the alpha GUI, you can start with this How-To-Guide .","title":"How to use the pre-alpha"},{"location":"how_to_guides/how_to_use_the_pre-alpha_code/#installing-the-pre-alpha","text":"Note: Our pre-alpha is frozen This will install the latest & last version from the pre-alpha phase of this project, frozen at release tag v0.0.54 here Open an Anaconda-enabled command prompt or powershell window and enter the following commands: 1) Create a Python 3.7 Anaconda environment conda create -n freemocap-env python = 3 .7 2) Activate that newly created environment conda activate freemocap-env 3) Install freemocap (version 0.0.54 ) from PyPi using pip pip install freemocap == 0 .0.54 Warning: BUG FIX - Update mediapipe with pip install mediapipe --upgrade That should be it!","title":"Installing the pre-alpha"},{"location":"how_to_guides/how_to_use_the_pre-alpha_code/#how-to-create-a-new-pre-alpha-recording-session","text":"tl;dr- Activate the freemocap Python environment and run the following lines of code (either in a script or in a console) import freemocap freemocap . RunMe () Be cool, use Blender COOL KIDS will install Blender ( blender.org and generate an awesome .blend file animation by setting useBlender=True : import freemocap freemocap . RunMe ( useBlender = True ) For additional, more detailed instructions (including methods to re-process recorded sessions), refer to the OLD_README.md document )","title":"How to create a new pre-alpha recording session"},{"location":"roadmap/roadmap/","text":"Roadmap \u00b6 In preparation for v0.1.0 alpha GUI release \u00b6 Fix the Blender output Probably going to build something based on the methods developed in @cgtinker's BlendArMocap add-on link to notes on this topic Fix the cameras alpha GUI starts lagging after about 3 cameras on a fast PC because cameras are run by threads (which compete for CPU with the GUI, vs the pre-alpha where it was all Terminal) Gonna implement a smarter method of connecting to cameras that involves splitting them across separate processes got some promising stuff on freemocap/fast-camera-capture , it just needs some more testing and then we can implement it in the GUI Planned work after v0.1.0 alpha GUI release \u00b6 UI/UX Workflow improvements Improve and validate the post processing pipeline Improve, streamline and develop validations/tests/diagnostics for: Gap-filling current method - Linear interpolation Filtering current method - Zero-lag 4th order low-pass Butterworth filter with cut off at, like, 7Hz Origin Align current method - a variety of things that don't work as well as I'd like, lol Try to automatically place skeleton on origin with feet on 'ground' (i.e. the XY plane) Add automated test suite (via GitHub Actions) for folder structure (e.g. 'completeness') for accuracy of post processing pipeline (e.g. 'kinematic diagnostics') for camera recording quality (e.g. 'timestamp diagnostics') New architecture for TUI Mothership method to develop alongside the standalone QtGUI Simple textual/TUI mothership terminal based app that launches qt wizards for each recording/processing stage Basically: it's just a simple launcher we can ask contributors to develop add-ons/plug-ins/improvements via QWidget objects that can launch from a widget.show() method. That way we can test/develop new capactites and organize them via the TUI interface before trying to integrate into the core freemocap GUI Kinda like a 'plug-in' system Kinda like pyqthgraph 's examples thing Keeps things nice and modular and easy to develop/collaborate Lets higher XP folks make tools that Lower XP folks can use/test/learn!! The Road to Real-time \u00b6 Desired workflow \u00b6 NOTE - This is the DESIRED workflow, not the current one lol Install and launch FreeMoCap software \u00b6 Go to freemocap.org/downloads Download and install the latest version of FreeMoCap for your OS Run installation wizard Double click on Skelly icon and launch the GUI Set up Cameras \u00b6 Plug them into the PC Set them up so they are pointing at the subject with sufficient lighting and overlapping fields of view Calibrate capture volume \u00b6 Show charuco board to each camera, each camera has shared views of the same board with at least one other cameras Press 'record/stream' or whatever \u00b6 A little thing pops up with info/diagnostics about the stream (IP address, success rate, framerate on both sides, system load info, etc) Connect to another 3d software (say Blender ) \u00b6 You see a rigged humanoid mesh pop up in a 3d viewport and the skeleton matches your movement (with max possible accuracy and min possible latency) Capacities needed \u00b6 Install \u00b6 standalone installers for Windows, Mac and Linux downloadable from freemocap website auto-build and update versions via Github Actions Core software \u00b6 improve workflow and UX as much as possible clean and sanitize skeleton output as much as possible minimize/optimize/distribute computational load as much as possible (and provide users with 'Speed vs Accuracy' knob based on their needs (and try to automate this process like they do in video game?)) Handle camera synchronization/processing in real-time Format skeleton in some intelligent way and stuff it into a Queue/PIPE that connects to a socket or something External apps \u00b6 Blender/Unreal/Unity/Gadot (or whatever) opens a socket and listens for skeleton data Skeleton data drives some 3d human skeleton/model/avatar/whatever And that's realtime baybee \u00b6 That's the dream, anyway. Expected timeline \u00b6 \"Soon\" \u00af\\_(\u30c4)_/\u00af","title":"Roadmap"},{"location":"roadmap/roadmap/#roadmap","text":"","title":"Roadmap"},{"location":"roadmap/roadmap/#in-preparation-for-v010-alpha-gui-release","text":"Fix the Blender output Probably going to build something based on the methods developed in @cgtinker's BlendArMocap add-on link to notes on this topic Fix the cameras alpha GUI starts lagging after about 3 cameras on a fast PC because cameras are run by threads (which compete for CPU with the GUI, vs the pre-alpha where it was all Terminal) Gonna implement a smarter method of connecting to cameras that involves splitting them across separate processes got some promising stuff on freemocap/fast-camera-capture , it just needs some more testing and then we can implement it in the GUI","title":"In preparation for v0.1.0 alpha GUI release"},{"location":"roadmap/roadmap/#planned-work-after-v010-alpha-gui-release","text":"UI/UX Workflow improvements Improve and validate the post processing pipeline Improve, streamline and develop validations/tests/diagnostics for: Gap-filling current method - Linear interpolation Filtering current method - Zero-lag 4th order low-pass Butterworth filter with cut off at, like, 7Hz Origin Align current method - a variety of things that don't work as well as I'd like, lol Try to automatically place skeleton on origin with feet on 'ground' (i.e. the XY plane) Add automated test suite (via GitHub Actions) for folder structure (e.g. 'completeness') for accuracy of post processing pipeline (e.g. 'kinematic diagnostics') for camera recording quality (e.g. 'timestamp diagnostics') New architecture for TUI Mothership method to develop alongside the standalone QtGUI Simple textual/TUI mothership terminal based app that launches qt wizards for each recording/processing stage Basically: it's just a simple launcher we can ask contributors to develop add-ons/plug-ins/improvements via QWidget objects that can launch from a widget.show() method. That way we can test/develop new capactites and organize them via the TUI interface before trying to integrate into the core freemocap GUI Kinda like a 'plug-in' system Kinda like pyqthgraph 's examples thing Keeps things nice and modular and easy to develop/collaborate Lets higher XP folks make tools that Lower XP folks can use/test/learn!!","title":"Planned work after v0.1.0 alpha GUI release"},{"location":"roadmap/roadmap/#the-road-to-real-time","text":"","title":"The Road to Real-time"},{"location":"roadmap/roadmap/#desired-workflow","text":"NOTE - This is the DESIRED workflow, not the current one lol","title":"Desired workflow"},{"location":"roadmap/roadmap/#capacities-needed","text":"","title":"Capacities needed"},{"location":"roadmap/notes/2022-11-13_blender_addon_notes/","text":"freemocap blender addon \u00b6 Strategy \u00b6 Based on fork of - https://github.com/cgtinker/blendarmocap figure out what was happening on freemocap branch of above Revert to main branch, then build new freemocap-auto-blender-output script based on main.py TO DO - [ ] auto-download sample data - [x] why skelly face down? - b/c he swaps XYZ -> -XZ-Y in the body, hand, face _processing.py files - [-] don't load live - can replace load_freemocap with better version (the existing one hijacks the holistic processor, and that's not necessary) - [ ] load body hands and face separately - [ ] bind other parts of rig to data - [ ] Remove hard-coded numbers in the processing functions e.g. pose_processing.py Line214","title":"freemocap blender addon"},{"location":"roadmap/notes/2022-11-13_blender_addon_notes/#freemocap-blender-addon","text":"","title":"freemocap blender addon"},{"location":"roadmap/notes/2022-11-13_blender_addon_notes/#strategy","text":"Based on fork of - https://github.com/cgtinker/blendarmocap figure out what was happening on freemocap branch of above Revert to main branch, then build new freemocap-auto-blender-output script based on main.py TO DO - [ ] auto-download sample data - [x] why skelly face down? - b/c he swaps XYZ -> -XZ-Y in the body, hand, face _processing.py files - [-] don't load live - can replace load_freemocap with better version (the existing one hijacks the holistic processor, and that's not necessary) - [ ] load body hands and face separately - [ ] bind other parts of rig to data - [ ] Remove hard-coded numbers in the processing functions e.g. pose_processing.py Line214","title":"Strategy"},{"location":"terminology/terminology/","text":"Terminology \u00b6 Capture Volume \u00b6 3-dimensional area (volume) with sufficient camera coverage to support 3D tracking. Calibration \u00b6 Link to a section of the 'braindump' video discussing capture volume calibration Charuco Board \u00b6 Link to a section of the 'braindump' video discussing capture volume calibration Mediapipe \u00b6 https://google.github.io/mediapipe/solutions/holistic Processing Stages \u00b6 This is a brief description of each of the processing 'stages' necessary to use a bunch of USB webcams to reconstruct the 3D kinematic (i.e. mocap) data of the human subject! Some parts refer to the folder and function names of the pre-alpha version of the code, but the concepts are mostly the same in the alpha version. Stage 1 - Record Videos Record raw videos from attached USB webcams and timestamps for each frame Raw Videos saved to FreeMoCap_Data/[Session Folder]/RawVideos Stage 2 - Synchronize Videos Use recorded timestamps to re-save raw videos as synchronized videos (same start and end and same number of frames). Videos saved to Synchronized Videos saved to FreeMoCap_Data/[Session Folder]/SynchedVideos Stage 3 - Calibrate Capture Volume Use Anipose 's Charuco-based calibration method to determine the location of each camera during a recording session and calibrate the capture volume Calibration info saved to [sessionID]_calibration.toml and [sessionID]_calibration.pickle Stage 4 - Track 2D points in videos and Reconstruct 3D <-This is where the magic happens \u2728 Apply user specified tracking algorithms to Synchronized videos (currently supporting MediaPipe, OpenPose, and DeepLabCut) to generate 2D data Save to FreeMoCap_Data/[Session Folder]/DataArrays/ folder (e.g. mediaPipeData_2d.npy ) Combine 2d data from each camera with calibration data from Stage 3 to reconstruct the 3d trajectory of each tracked point Save to /DataArrays folder (e.g. openPoseSkel_3d.npy ) NOTE - you might think it would make sense to separate the 2d tracking and 3d reconstruction into different stages, but the way the code is currently set up it's cleaner to combine them into the same processing stage \u00af\\_(\u30c4)_/\u00af Stage 5 - Use Blender to generate output data files (optional, requires Blender installed. set freemocap.RunMe(useBlender=True) to use) Hijack a user-installed version of Blender to format raw mocap data into a .blend file including the raw data as keyframed emtpies with a (sloppy, inexpertly) rigged and meshed armatured based on the Rigify Human Metarig Save .blend file to [Session_Folder]/[Session_ID]/[Session_ID].blend You can double click that .blend file to open it in Blender. For instructions on how to navigate a Blender Scene, try this YouTube Tutorial Stage 6 - Save Skeleton Animation! Create a Matplotlib based output animation video. Saves Animation video to: [Session Folder]/[SessionID]_animVid.mp4 Note - This part takes for-EVER \ud83d\ude05 Reprojection Error \u00b6 \"Reprojection error\" is the distance (in pixels?) between the originally measured point (i.e. the 2d skeleton) and the reconstructed 3d point reprojected back onto the image plane. The intuition is that if the 3d reconstruction and original 2d track are perfect, then reprojection error will be Zero. If it isn't, then there is some inaccurate in either: the original 2d tracks (i.e. bad skeleton detection in one or more cameras), in the 3d reconstruction (i.e. bad camera calibration), a combination of the two Click here to follow a conversation about reprojection error on discord","title":"Terminology"},{"location":"terminology/terminology/#terminology","text":"","title":"Terminology"},{"location":"terminology/terminology/#capture-volume","text":"3-dimensional area (volume) with sufficient camera coverage to support 3D tracking.","title":"Capture Volume"},{"location":"terminology/terminology/#calibration","text":"Link to a section of the 'braindump' video discussing capture volume calibration","title":"Calibration"},{"location":"terminology/terminology/#charuco-board","text":"Link to a section of the 'braindump' video discussing capture volume calibration","title":"Charuco Board"},{"location":"terminology/terminology/#mediapipe","text":"https://google.github.io/mediapipe/solutions/holistic","title":"Mediapipe"},{"location":"terminology/terminology/#processing-stages","text":"This is a brief description of each of the processing 'stages' necessary to use a bunch of USB webcams to reconstruct the 3D kinematic (i.e. mocap) data of the human subject! Some parts refer to the folder and function names of the pre-alpha version of the code, but the concepts are mostly the same in the alpha version. Stage 1 - Record Videos Record raw videos from attached USB webcams and timestamps for each frame Raw Videos saved to FreeMoCap_Data/[Session Folder]/RawVideos Stage 2 - Synchronize Videos Use recorded timestamps to re-save raw videos as synchronized videos (same start and end and same number of frames). Videos saved to Synchronized Videos saved to FreeMoCap_Data/[Session Folder]/SynchedVideos Stage 3 - Calibrate Capture Volume Use Anipose 's Charuco-based calibration method to determine the location of each camera during a recording session and calibrate the capture volume Calibration info saved to [sessionID]_calibration.toml and [sessionID]_calibration.pickle Stage 4 - Track 2D points in videos and Reconstruct 3D <-This is where the magic happens \u2728 Apply user specified tracking algorithms to Synchronized videos (currently supporting MediaPipe, OpenPose, and DeepLabCut) to generate 2D data Save to FreeMoCap_Data/[Session Folder]/DataArrays/ folder (e.g. mediaPipeData_2d.npy ) Combine 2d data from each camera with calibration data from Stage 3 to reconstruct the 3d trajectory of each tracked point Save to /DataArrays folder (e.g. openPoseSkel_3d.npy ) NOTE - you might think it would make sense to separate the 2d tracking and 3d reconstruction into different stages, but the way the code is currently set up it's cleaner to combine them into the same processing stage \u00af\\_(\u30c4)_/\u00af Stage 5 - Use Blender to generate output data files (optional, requires Blender installed. set freemocap.RunMe(useBlender=True) to use) Hijack a user-installed version of Blender to format raw mocap data into a .blend file including the raw data as keyframed emtpies with a (sloppy, inexpertly) rigged and meshed armatured based on the Rigify Human Metarig Save .blend file to [Session_Folder]/[Session_ID]/[Session_ID].blend You can double click that .blend file to open it in Blender. For instructions on how to navigate a Blender Scene, try this YouTube Tutorial Stage 6 - Save Skeleton Animation! Create a Matplotlib based output animation video. Saves Animation video to: [Session Folder]/[SessionID]_animVid.mp4 Note - This part takes for-EVER \ud83d\ude05","title":"Processing Stages"},{"location":"terminology/terminology/#reprojection-error","text":"\"Reprojection error\" is the distance (in pixels?) between the originally measured point (i.e. the 2d skeleton) and the reconstructed 3d point reprojected back onto the image plane. The intuition is that if the 3d reconstruction and original 2d track are perfect, then reprojection error will be Zero. If it isn't, then there is some inaccurate in either: the original 2d tracks (i.e. bad skeleton detection in one or more cameras), in the 3d reconstruction (i.e. bad camera calibration), a combination of the two Click here to follow a conversation about reprojection error on discord","title":"Reprojection Error"}]}